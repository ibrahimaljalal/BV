{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Go to the About and the User Guides pages to learn more about the software.</p>"},{"location":"FAQs/","title":"FAQs","text":"Will there be additional projects to the three projects in this software? <p>Although it is relatively easy to add other projects, there is yet no official commitment to do so for multiple of reasons. So it is possible that this software will not receive any further updates in the future.</p> Who could benefit from this software? <p>Since BV software is a platform that could have multiple projects and could scale up to more than the current three projects, we generally expect three types of users:</p> <ol> <li> <p>University Students: By using the \"Generate Code\" feature, students could configure or add their own code to the projects as they like (for more information go to the \"Generate Code\" section in the User Guides). </p> </li> <li> <p>Companies: From the currant projects, The main use for this software is in surveillance applications. Examples are Compounds, Malls, &amp; in some drone application such as oil leakage detection.</p> </li> <li> <p>Normal Users: The currant three projects unfortunately might not be very appealing for these types of users however we might add a project where you could use Deepfake such as in the two videos below: video 1 video 2 The idea is that we would have an already prepared video footage and you simply add your picture or anyone's else and leave the rest for us. As stated in the previous answer that there may not be additional projects. Furthermore a similar project such as this one might not be as easy as it seems.</p> </li> </ol> Will the three projects always be free? <p>We guarantee that they will be free until the end of 2022.</p>"},{"location":"about/","title":"About","text":"Brilliant Vision (BV) is a software which contains multiple independent projects related to computer vision applications. It was made while keeping three features in mind:     1. Make it easy for anyone to use the applications related to computer vision.     2. Scalability &amp; fast updates. The software has currently three independent projects but because the main infrastructure of the software was established, additional projects could be added easily. This is very useful because if for an example an AI model or any useful code related to computer vision has just been recently published, then it could be immediately integrated to this software and hence used by the general public.     3. Give Python developers the ability to configure or add they\u2019re code on certain projects (go to \"Generate Code\" section in the User Guides for more information). The generated code will be an additional layer of abstraction for computer vision specialists."},{"location":"best_practices/","title":"Best Practices","text":"prerequisites <ol> <li>About page.  </li> <li>User Guides page (especially the \"General Guide\" and \"Control Window\" sections). </li> </ol> <p>In order to comfortably use the software and avoid some issues then these are our recommendations:</p> <p>1-We recommend that you at least have two monitors. This would be more needed if you are going to use the color detection or pipeline detection projects since they will show 6 display windows.  </p> <p>2-Although you can use multiple control windows at the same time we recommend that you only use one.  </p> <p>3-Do not click on the \"Start\" button in the \"Setup Window\" more than once before the project is opened.  </p> <p>4-Do not use a very slow computer (An average computer with no GPU could do fine).  </p> <p>5-Do not refresh the \"Control Window\" because if you do you will not see the newly add settings, but do not worry they will not be remove instead they will be available in the next time you open the project.  </p>"},{"location":"documentation/","title":"Documentation Introduction","text":"prerequisites <ol> <li>About page.  </li> <li>User Guides page. Especially the Generate Code sections (very important).    </li> <li>Basic understanding in the Python programing language.  </li> <li>Basic understanding in the OpenCV library for Python.  </li> <li>Basic understanding in the JSON file formate.  </li> </ol> important notes <p> 1-The name of the variables in the .json files in the \"settings\" folder could also be used in the code in the main.py file. for example if you are using the object detection project then you could write object_detection.AI_model = \"YOLO_v3_accurate\" or object_detection.confidence = 90. The main two exceptions are the input_type and input variables. The variables are available but you might get errors so it is preferred to select there values in the .json file instead.   <p>2-When the code is generated the \"input_type\" and \"input\" values will always be \"camera\" and 0 respectively if the user did not make his own changes in the .json file.  </p> <p>3-The documentation is mainly about the .json files (in the \"settings\" folder for the generated code) and the results (and very briefly the results_frames) that can be used in the main.py file (in the generated code).  </p> <p>4-This is how we use the results and results_frames in the main.py file:</p> main.pysettings_file_name.txt <pre><code>#this is an example for the Object Detection project. \n#if you use another project, then simply \n#replace \"object_detection\" \n#with the project name\nimport object_detection\nimport cv2 as cv\nfrom codes_library import reference as r\nJSON_file_name = object_detection.get_settings_file_name()\nobject_detection.load_settings(JSON_file_name+\".json\")\ninput_object = r.InputFrom(input_type=3, input=0)\nframe = input_object.read_input()\n[results,results_frame] = object_detection.detect(frame)\n</code></pre> <pre><code>default\n</code></pre>"},{"location":"downloads/","title":"Downloads","text":"<p> BV Presentation </p> <p> All Files </p>"},{"location":"examples/","title":"Introduction","text":"prerequisites <ol> <li>About page.  </li> <li>User Guides page (especially the \"General Guide\" and \"Control Window\" sections).  </li> </ol> notes <p> 1. Most of the illustrations will have their explanation immediately after them (Some might be self-explanatory, but we have added explanations just in case). 2. Some of the illustrations in the examples are available in a folder called resources. This is the link of the folder (the link might not be available in the future). The purpose of this folder is to enable you to test some of the features of the software yourself. </p>  This section will explain some of the parameters for the three projects in the software. Unlike the Results section, this section may not have real life examples and most of the illustrations will have comments to explain what is going on since its main purpose is to make you understand the settings in the control window."},{"location":"feedback/","title":"Feedback","text":"<p> <p></p> Browser not compatible. <p></p>"},{"location":"others/","title":"Uncompleted Projects","text":""},{"location":"others/#1-3d-website","title":"1-3D Website:","text":"<p>b3d </p>"},{"location":"others/#models-in-the-website","title":"Models in the Website:","text":"<p>Cake Burger Steak</p> <p>Interior Design 1 Interior Design 2</p> <p>Meat Grinder Robot</p>"},{"location":"others/#2-semi-autonomous-car","title":"2-Semi Autonomous Car:","text":"<p>Link </p>"},{"location":"others/#3-kfupm-sce-578-human-robot-interaction","title":"3-KFUPM - SCE 578 - Human Robot Interaction:","text":"<p>Link </p>"},{"location":"results/","title":"Introduction","text":"<p>In this section unlike the Examples section, it  will not include any explanation but only the results and some brief comments in some results which most of them are related to real life applications.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"prerequisites <ol> <li>About page.  </li> <li>User Guides page (especially the \"General Guide\" and \"Control Window\" sections). </li> </ol> Image or video file not opening after clicking \"Start\" in the \"Setup Window\" <p>Simply change the name of the file to 00 then a number. For example 001.png, 007.mp4, 002.mkv etc. This issue is related to the opencv library.</p> Camera takes a lot of time to open after I click \"Start\" in the \"Setup Window\" <p>This is probably because the camera you are using is an external camera and it is not installed correctly. If you use your internal camera for your laptop (write 1 after the \"Input Type\" field after you have selected \"Camera\" in the \"Setup Window\") the issue should not happen. Sometimes the external camera takes a lot of time (around 30 seconds for an average computer). This might be because of the way the camera was installed or because of the driver.</p>"},{"location":"user_guides/","title":"User Guides","text":"prerequisites <ol> <li>About page.   </li> <li>In the Generate Code section you should have some experience with: a. Python programing language. b. OpenCV library for python. c. JSON file formate. d. Some experience with linux operating system &amp; the Raspberry Pi microprocessor. (This part is optional).  </li> </ol> notes <p> 1. Most of the illustrations will have their explanation immediately after them (Some might be self-explanatory, but we have added explanations just in case). 2. Some of the demonstrational examples used in this User Guides are available in a folder called resources. This is the link of the folder (the link might not be available in the future). The purpose of this folder is to enable you to test some of the features of the software yourself. 3. In general there is no prerequisites for the \"User Guides\" since one of the main goals of this software is to make it usable by almost anyone (the \"Generate Code\" section is the main exception). </p> General Guide <p></p> <p> 1-The license window will by default contain the free license.   <p>2-After accessing the software you will be presented with three projects each one with its brief description (There might be much more in the future!). In this window (and in the license window) if you have noticed some issues in the software, you could click on \"Report an Issue\" button and explain exactly what is your issue or give your feedback on github.com. Or click on \"Website\" to get more information related to the documentation, FAQs, troubleshot, best practices, etc.  </p> <p>3-The \"Setup Window\" will be the same for all projects. The most important parameter is the \"Input Type\" and its corresponding input. The input types are Camera, Image, Video, and IP Address (Image and Video are usually used for testing purposes).  </p> <p>4-After clicking the \"Start\" button in the \"Setup Window\", a window named \"Control Window\" will open with another window (or in some projects multiple windows) showing the results.</p> <p>4.1-The Control Window will have all the parameters or settings related to the project which has been selected. Note that some parameters or settings are common for all projects.  </p> <p>4.2-The Display Window (or multiple windows) will display the results (note that its title will depend on the context of the display). If there are multiple windows, they will be numbered chronologically according to the operation the frame goes through so that the user would have more understanding on what is going on. </p> Control Window <p></p> <p> As it has been explained in the \"General Guide\" section, the control window will open with another window (or multiple windows depending on the project). The Control window will have all the main parameters related to the project you have selected. In the previous slide we have selected the \"Object Detection\" project. The \"Input Type\" was \"Image\", and the image is in the \"resources\" folder (should come with the software when you download it so that you could test for yourself). If you want even more control such as changing the font on the display window you can do that in the .json file after generating the code (to know more about that go to the \"Generate Code\" section which is mainly for developers). </p> <p> </p>"},{"location":"user_guides/#control-window-features","title":"Control Window Features:","text":""},{"location":"user_guides/#1question-mark","title":"1.Question Mark","text":"<p> Some parts in the control window which may not be very clear, needs more context, or additional information will have the \"?\" mark near it to give further explanation. The Explanation is usually an image or text (we might add video in the future if necessary). </p>"},{"location":"user_guides/#2common-settings","title":"2.Common Settings","text":""},{"location":"user_guides/#part-1","title":"Part 1","text":"<p> <p>  The first part in the \"Common Settings\" section is perhaps the most important part of the three parts because it might substantially improve your time and help you organize your  work. When you have tweaked the settings in the control window to your needs and want to use the same settings later, then simply write any name you like in the \"New Settings Name\" field (it is recommended to clearly describe your settings name just like the in the demonstration) and then click \"Save Settings\". After that, the name you have entered will be available in the above dropdown menu. </p> <p> When \"Show Frames\" is off, the display will freeze but when you generate the code (go to \"Generate Code\" section to learn more) and run the code, then the display will not open at all. This will be helpful if you are plaining to use the code in the Raspberry Pi microprocessor in your robot project or any type of project since displaying the frames will waste more computer recourses. </p>"},{"location":"user_guides/#part-2","title":"Part 2","text":"<p> Part 2 is simply the location of the footage. This might be useful if for example you have installed hundreds of cameras for a surveillance application, and they are all sending the results to a free google drive account, then definitely you would like to know where each result was taken. Another example is if you have a drone, and you want to know the exact location a certain object was detected by providing the x, y, and maybe z coordinates. In this case you need to dynamically change this variable and to do so you need to generate the code and program it to your needs. </p>"},{"location":"user_guides/#part-3","title":"Part 3","text":"<p> After choosing the place you want to save your results in the setup window in point 1 and checking on the \"Save Results\" in point 2, then all the results in detail will be saved in a csv file (point 3) which will have the same name as the project name. The next demonstration shows how the csv file works. <p>Note: you will not be able to see the results live if you open the file with Excel. So, one way to see the results live is to use the \"visual studio code\" editor software (you can easily download it since it is free and open source) and optionally install the \"Rainbow CSV\" extension to have similar results as in the demonstration. </p> <p> <p>All the details of a detection will be saved in a .csv file.</p> <p></p> <p></p> <p> The csv file could be extremely important in case for an example you do not have hundreds but thousands of cameras and all of them are sending the detected results to one free google drive account. If you want to send the results as images this might be nearly impossible even with a fast internet since media files take a lot of bandwidth. This is not the case when you only want to send the description of the image which was detected (it will have much lower size then the image itself). One interesting way to utilize the results in the csv file is to take the location coordinates of a detected object and display it on a map just like the image above. Note that the image is not an example from the software but a possible addition that could be easily added by the developer who generated the code. </p> <p></p> <p> The \"Frames Skipped\" will be useful if you do not want to have a lot of images saved in you drive. If for an example you make it 100, then from the first time it detects something it will save it as an image and waits 100 frames after that the program will save again if there is something detected and waits 100 frames and so on. <p>Notes: 1. The count includes the detected image so if something is detected at frame 1 and it will continue to be there then the next save will be in frame 100 not 101. 2. For the object detection project every object has its own separate dimension. Meaning that the first time a cat is detected is not like the first time a dog is detected. For an example if a dog, and a cat were detected at frame 1 and they remained being detected and an apple was detected at frame 2 and remained, and a remote was detected at frame 4 and remained and the \"Frames Skipped\"  was set to 100 then the images which are being saved are 1, 2, 4, 101, 102, 104, 201, 202, 204 and so on. That make sense and should be more  convenient for the user because if all objects where treated the same then some objects may get detected but never be saved as an image. 3. The \"Frames Skipped\"  will not be applied to the csv file since it does not waste a lot of storage.</p> <p>The \"Save Images\" is self explanatory. If it is off, then the images will not be saved in the \"images\" folder. </p> <p></p> Generate Code <p>  Please note that this section is meant for developers  who have some experience with the Python programing language. If you are not a developer, then you could still go through the steps and configure the .json file since you do not need to be a programmer to understand how to read or write on a .json file (few YouTube tutorial should give you the general idea), but the main purpose of this section is to enable programmers to add this code as part of their project and make them configure it the way they like. <p></p> <p>1-Select the path you want the project to be generated to.  </p> <p>2-The currant settings is named \"default\". You can change it from the menu or make your own setting. You do not need to check off the \"Show Frames\" but if it is turned on and your plaining to make this code run In the Raspberry Pi immediately after it is powered on (the instructions for that are in the read.md file), then the code will not work. The \"Generate Code\" button will make a folder with the setting selected in our case it is \"default\".  </p> <p>3-After clicking \"Generate Code\" in part 2 a folder will be made in the selected path from part 1.  </p> <p>4-After entering the generated folder (in our case it is named \"object_detection\" from part 3) we will see 5 folders and files. Here are the files and folders we should be concern of: a-readme.md, b-main.py, c-settings_file_name.txt, d-settings folder, e-Results folder.  The rest we should not worry about . Even from a to e the only ones we could manually make changes to are b-main.py, c-settings_file_name.txt, and d-settings folder (the .json files) the reaming two are for reading only. Note that the illustration we have used is for the object detection project, but these steps should work the same for the remaining projects and possible future projects.</p> <p>Here are the main files/folders we should consider in any generated code from the BV software: a. readme.md: this file will have all the instructions for running the code b. main.py: the starting point of the program. after going through the readme.md file you could run the code and it should work. For the rest of the files/folders (4.c, 4.d, and 4.e) you do not need to even open them for the program to work but sometimes it is useful to make some changes which suits your needs. c. settings_file_name.txt: This is the file which is opened in the right window in part 4. In it you put the name of the setting you want to use. In our case because we generated the code, and the settings was on \"default\" in part 2 then \"default\" is what is in this file. if the setting name in part 2 is test then the value test will be in this file. d. settings folder: All the setting files (.json files) are in this folder and one of them is \"default\". Note that all the files (.json files) should have the same structure. The possible values and meaning of each variable is in the documentation section of the website: https://ibrahimaljalal.github.io/BV/documentation (the domain might change in the future). An example of how to use the .json file is in the next illustration. Note that in OpenCV unlike most software the color channels are (b,g,r) instead of (r,g,b). e. Results folder: This folder will simply save any detection in your project.  </p> <p>5-In case you want to use this code on a Raspberry Pi make sure you use version 4 with 64-bit architecture and the operating system is \"Raspberry Pi OS\" (64-bit). Make sure to install the 64-bit OS not the 32-bit OS  (before 2022 64-bit was not available and 32-bit might still be the default)</p> <p> <p>A brief demonstration on how to use of the .json file</p> Connect Other Devices"},{"location":"user_guides/#1-stream-to-the-software","title":"1. Stream to the Software","text":"<p>If you have an Android Device, then use the \"IP Webcam\" app to stream to the software. The instructions will be available in detail (not shown in the image) once you select the \"IP Address\" as an \"Input Type\" in the \"Setup Window\". Make sure you are at the same local area network (LAN). The type of communication is TCP which means we should not worry about corruption in the footage. The speed of the TCP in the LAN network is fast so there is currently no need for UDP in this software.</p>"},{"location":"user_guides/#2-connect-to-the-control-window","title":"2. Connect to the Control Window","text":"<p> <p>If you are at the same local area network (LAN) then you can allow other devices from the network to access the \"Control Window\" of the software. Note that you could connect as many devices as you like but it is recommended to use one. </p>"},{"location":"documentation/color_detection/","title":"Color Detection","text":"notes <p>This project is almost exactly identical to the pipe_angle project (that was not intentional but for multiple of reasons we have decided to separate them instead of making them a single project).</p> .json file input_type input centroid_size <p>type: float possible values: from 0 to 1 default value: 0.015  </p> <p>Explanation: the size of the radius of the circle which is the centroid of the largest continuous white area (the white area is the detected color range)</p> centroid_color <p>type: list possible values: the list has three values each could be from 0 to 255 default value: [0,0,255]  </p> <p>Explanation: the bgr color (Blue,Green,Red) of the centroid circle</p> contour_s_color <p>type: list possible values: the list has three values each could be from 0 to 255 default value: [0,0,255]  </p> <p>Explanation: the bgr color (Blue,Green,Red) of the border (or perimeter) of the largest continuous white color (the color that represent the detected range)</p> contour_s_thickness <p>type: float possible values: from 0 to 1 default value: 0.01  </p> <p>Explanation: the thickness of the border (or perimeter) of the largest continuous white color (the color that represent the detected range).  the value is the ratio of the frame hight.</p> lowerH, upperH, lowerS, upperS, lowerV, and upperV <p>type: int possible values: from 0 to 360 for lowerH, and upperH. and from 0 to 256 for lowerS, upperS, lowerV, and upperV  </p> <p>Explanation: the lower and upper ranges for the HSV color (HSV stands for hue saturation value)  </p> <p>for more information go to: http://color.lukas-stratmann.com/color-systems/hsv.html or https://medium.com/programming-fever/how-to-find-hsv-range-of-an-object-for-computer-vision-applications-254a8eb039fc  </p> additional_lowerH, additional_upperH, additional_lowerS, additional_upperS, additional_lowerV, and additional_upperV <p>type: list possible values: from 1 to 360 for lowerH, and upperH. and from 1 to 256 for lowerS, upperS, lowerV, and upperV. all of the list should have the same number of inputs or you will get an error.  </p> <p>Explanation: an additional HSV color range (HSV stands for hue saturation value). this will be useful if you want to select two more different colors at the same time.</p> <p>Notes: please note that the lists should have the same number of values or you will get an error. for example if additional_lowerH = [2,77,300,44] (5 inputs), then the rest should have 5 inputs.  </p> <p>for more information go to: http://color.lukas-stratmann.com/color-systems/hsv.html or  https://medium.com/programming-fever/how-to-find-hsv-range-of-an-object-for-computer-vision-applications-254a8eb039fc  </p> blur <p>type: int possible values: from 0 and more (preferred less then 20) default value: 4  </p> <p>Explanation: This filter the image and makes it blur. this is one of the filters that will help to get ride of the noise  </p> erosion <p>type: int possible values: from 0 and more (preferred less then 10) default value: 5  </p> <p>Explanation: This filter will decrease all of the white areas which represent the detected color range. this is one of the filters that will help to get ride of the noise.  </p> dilation <p>type: int possible values: from 0 and more (preferred less then 10) default value: 2  </p> <p>Explanation: This filter will increase all of the white areas which represent the detected color range. this filter will be applied after the erosion filter. it will be useful if for example there are two close parallel objects such as two pips and we want to analyze the two as a one object. also this will solve the issue of having two objects with almost identical areas which will make the program alternate between the two objects.  </p> contourAreaRatio <p>type: float possible values: from 0 to 1 (preferred 0.005) default value: 0.005  </p> <p>Explanation: if the ratio of the white color pixels (the detected color range) to the frame (hight * width) pixels is greater than this value then it will start recording the detection.</p> show_frames <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: if false the display window will not show. this will be useful if you want to use less processing power.  </p> <p>Notes: if you want to use the code for a certain project with the Raspberry pi 4 (or any lny linux machine) and would like the code to immediately run once you power the processor, then follow the steps in the readMe.md file which will be generated if you click on the \"Generate Code\" in the \"Control Window\" and make sure to make this variable false.</p> resize scale delay_time ASCII_close_window_key location <p>type: string possible values: any string default value: KFUPM  </p> <p>Explanation: This variable will be one of the results that will be recorded once a detection was found.  </p> <p>Use case: one of the useful cases is when this software is used in a surveillance application. Say that you decided to use 1000 cameras in a certain area. multiple cameras or each one is connected to a computer that uses the software. you can make the location variable for each camera describes exactly the place it is in such as using the x and y GPS coordinate. Another example is if you have a drone you could update the location variable as the drone moves so that you could know the exact coordinate of each detection you had.</p> font_TTF_file <p>type: string possible values: .ttf file path default value: resources/fonts/times.ttf (The Times New Roman Font)  </p> <p>Explanation: the font that will be used in displaying most of the texts in the software.  </p> display_text_sizes <p>type: python dictionary (or javascript object) with 5 variables each is a float possible values: every variable in display_text_sizes could be from 0 to 100 default value: {\"fps\":5,\"location\":5,\"SSD_objects\":5,\"date_and_time\":5,\"AI_model\": 5}  </p> <p>Explanation: the 5 variables will be displayed in the frame. their values represent their size percentage relative to the frame hight. so if the frame hight is 1000 pixels and for an example fps is 10 then it will be 10% of the hight of the frame which is 100 pixels height.</p> <p>Notes: if you want to remove one of the 5 texts displayed in your frame then make the its variable equals to 0.  </p> display_text_colors <p>type: python dictionary (or javascript object) with 5 variables each has a list of three integers possible values: every variable in the font_sizes has a list that has three values each could be from 0 to 255 default value: [255,0,0] for all (blue)  </p> <p>Explanation: the values in the list inside the variables inside the font_sizes describes the color of the text. the color order is BGR (Blue, green, Red). For example if fps is [0,0,255] then it will be red or if it is [0,255,255] it will be yellow and so on.</p> display_text_locations <p>type: python dictionary (or javascript object) with 5 variables each has a list of two floats possible values: every variable in font_sizes has a list that has two values each could be from 0 to 1  </p> <p>Explanation: the values in the list inside the variables inside the font_sizes describes the location of the texts.  </p> <p>the first value is the x axis and the second value is the y axis. the coordinate starts from the upper left. the x axis increases from left to right and the y axis increases from up to bottom. the x and y axes is for the location of the upper left of the text.  </p> <p>for example if fps is [0.5, 0.5] then the upper left point of the text will be at the center.  </p> <p>Note: if you make fps or any variable's x or y axis to 1 then it will disappear because this is the location of the upper left of the text.  </p> save_results <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: if there was a detection found then this variable will save the results in the Results folder.  </p> <p>Notes: a .csv file will always be present if this value is true. if this value is false then the save_images will not have effect even if it true.  </p> save_images <p>type: boolean possible values (depends on save_results): true or false default value: true  </p> <p>Explanation: if this variable is true then there will be images saved in the Results folder. make this variable false if you want to save space.  </p> <p>Notes: if the save_results variable is false then the save_images variable will not save images even if it ia true  </p> add_to_previous_results <p>type: boolean possible values: true or false default value: false  </p> <p>Explanation: if this value is true and you close and open the program multiple times then the .csv file in the Results folder will not change (the results will be accumulated)  </p> add_header_to_next_results <p>type: boolean possible values (depends on add_to_previous_results): true or false. default value: true  </p> <p>Explanation: this variable will add a header in the .csv file in the Results folder to the accumulated results (if add_to_previous_results is true and you have open the program more than two times).</p> image_extension <p>type: string possible values: jpg or png default value: jpg  </p> <p>Explanation: the formate of the image that will be saved. use .jpg if you want less size.  </p> add_results_on_image <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: if false the results images in the Results folder will not display some results such as frames per seconds and date and time, and location etc.  </p> <p>Notes: only the saved images will not show if this variable is false. the window that will display the  </p> results results[\"frame_number\"] <p>type: int  </p> <p>Example: 478  </p> <p>Example Explanation:  </p> <p>the frame number since the beginning of the program.  </p> results[\"fps\"] <p>type: float  </p> <p>Example: 5.58  </p> <p>Example Explanation:  </p> <p>the frames per second at the time of detection.  </p> results[\"location\"] <p>type: string  </p> <p>Example: KFUPM  </p> <p>Example Explanation: This is simply one of the inputs that is also used as an output for getting more information about the results.  </p> results[\"date_and_time\"] <p>type: string  </p> <p>Example: 2022/08/03, Wed, 04:46:40 PM  </p> <p>Example Explanation: the date and time there were detection on the frame. year/month/day, day name, hour:minute:seconds AM or PM  </p> results[\"x\"] <p>type: float  </p> <p>Example: 0.50  </p> <p>Example Explanation:  </p> <p>the normalized x coordinate of the centroid of the contour area (the white area which represent the color range). the 0.5 means that it is in the middle of the frame. the x axis starts from the left and increase right.</p> results[\"y\"] <p>type: float  </p> <p>Example: 0.9  </p> <p>Example Explanation:  </p> <p>the normalized y coordinate of the centroid of the contour area (the white area which represent the color range). the 0.9 means that it is almost at the top. the x axis starts from the bottom and increase upp</p> results_frames <p>type: numpy object.  </p> <p>possible values: three or sometimes two dimensional array (hight, width, and sometimes color channels with three values which are usually (b,g,r)). for the three or one channel (two dimensional array) the values are usually from 0 to 255 (uint8 type in the numpy library)  </p> <p>Notes:   1-the \"type\" and \"possible values\" are true for all of the BV software.   2-the names of the frames (or one frame) should be self explanatory so we will not go deep in explaining them. Some one with basic knowledge with the numpy library should be able to deal with them without issues.  </p> <p>frames:   results_frames[\"1_original_with_results\"]   results_frames[\"2_filtered\"]   results_frames[\"3_HSV_range(s)\"]   results_frames[\"4_erosion\"]   results_frames[\"5_dilation\"]   results_frames[\"6_contour(s)_centroid\"]  </p>"},{"location":"documentation/color_detection/#_1","title":"Color Detection","text":"<p>type: int or string (only in the *.json file in the settings folder) possible values (depends on input variable): 1 (or video), 2 (or image), 3 (or camera), 4 (or ip). default value: camera  </p> <p>Explanation: Determine what kind of input the program will use. There are four possible integer values for this variable. They are: 1 (video), 2 (image), 3 (camera), and 4 (ip). more details are in the input variable documentation.  </p> <p>Notes: For your convenience it is possible to use string type as an input but only in the *.json file in the settings folder. for example instead of writing 3 you could write camera or Camera in the default.json file. the load_settings function in your main.py file will take care of converting the input_type variable to an integer. </p> <p>Also note that the input variable depends on the input_type variable. So if you change the input_type variable you should make sure that the input variable will have a correct value. for example if the input_type variable is a camera (or number 3), then the input variable should only be an integer which represent which camera in your device such as 1. Another example if the input_type is video (or number 1) then the input should be a file path such as C:/filename.mp4</p>"},{"location":"documentation/color_detection/#_2","title":"Color Detection","text":""},{"location":"documentation/color_detection/#_3","title":"Color Detection","text":"<p>type: int, string possible values (depends on input_type variable): input_type is 1 or 2 (video or image): possible values are file paths (Example: C:/filename.mp4 or C:/filename.png). input_type is 3 (camera): possible values are integers from 0 or more (Example: 2). input_type is 4 (ip): possible values are Live stream URLs (Example: https://192.168.100.25:8080/video).  </p> <p>default value: 0 (for camera)  </p> <p>Explanation: This variable depends on the input_type variable. Here is a brief description of the variable:  </p> <p>input_type is video or image: The input will be a path of the image or video you want to use in you program. usually we use the video or image for testing purposes.  </p> <p>input_type is camera: The input variable will be an integer that will describe what camera you are using (note that the count starts from 0 not 1). So if you have three cameras on your device and write 2 in the *.json file, then you will use the third camera. if you write 3 then you will get an error.</p> <p>input_type IP: The live stream ip address (URL) you want to use in your program. you could stream from your phone if you use the IP Webcam App for Android devices. If you install the app, open it and scroll to the bottom and select \"Start server\". Your live IP server in the local area network will be after \"IPv4:\" (you could choose the first or second). the URL that you need to past is what is after \"IPv4:\" plus \"/video\" (Example: http://192.168.100.25:8080/video) make sure you are at the same local area network. you can test if the stream is available by simply writing the URL in your web browser such as Chrome or Firefox.</p> <p>Notes: If you are using Windows and the input is for an image or video use \"/\" instead of \"\\\" for the path of the video or image or add additional \"\\\" to \"\\\". For example C:\\Users\\Ibrahim\\filename.mp4 should instead be C:/Users/Ibrahim/filename.mp4 or C:\\Users\\Ibrahim\\filename.mp4</p> <p>Do not forget to add the extension. Example: filename.mp4 not filename  </p>"},{"location":"documentation/color_detection/#_4","title":"Color Detection","text":""},{"location":"documentation/color_detection/#_5","title":"Color Detection","text":"<p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: This variable simply will allow you to scale your frame. if it is false, then the scale variable will not have any effect</p>"},{"location":"documentation/color_detection/#_6","title":"Color Detection","text":"<p>type: float possible values (depends on resize variable): more than 0 and preferred to be less than 1 default value: 1  </p> <p>Explanation: This variable is the percentage scale of your original frame. if you have a frame of 1000 pixels width and 1000 pixels height and you make scale variable to 0.5 then you will have 500 pixels for height and width. this variable  will be useful if you have a frame with very high resolution and you want to make your program run faster.</p> <p>Notes: This variable depends on the resize variable. if it is false then its value will have no effect. It will not make sense if you make this variable more than 1 since it will not increase the resolution but simply will make your program much slower since you have increased the number of pixels.</p>"},{"location":"documentation/color_detection/#_7","title":"Color Detection","text":""},{"location":"documentation/color_detection/#_8","title":"Color Detection","text":"<p>type: int possible values: 1 and more (preferred 1) default value: 1  </p> <p>Explanation: time in milliseconds it waits after each frame. </p>"},{"location":"documentation/color_detection/#_9","title":"Color Detection","text":""},{"location":"documentation/color_detection/#_10","title":"Color Detection","text":"<p>type: int possible values: 0 to 127 (preferred 27) default value: 27  </p> <p>Explanation: the key on your keyboard that will close the program. every key on your keyboard has an ASCII value. the ASCII for the Esc key is 27</p>"},{"location":"documentation/color_detection/#_11","title":"Color Detection","text":""},{"location":"documentation/object_detection/","title":"Object Detection","text":".json file input_type input AI_model <p>type: int, string (only in the *.json file in the settings folder) possible values: 1 (or SSD), 2 (YOLO_v3_accurate), or 3 (YOLO_v3_not_accurate) default value: 1 (SSD)  </p> <p>Explanation: there are so far three AI models in this software. they are:  </p> <p>SSD: usually this is the best choice since it is relatively fast and is the fastest out of the three models and is generally more accurate than YOLO_v3_not_accurate model (or 3)</p> <p>YOLO_v3_accurate: This is based on the You Look Only Once AI model. this is the most accurate model of the three but its main disadvantage is that it is very slow. if you have a normal PC it could in some worst cases get as slow as 0.1 frames per seconds.  </p> <p>YOLO_v3_not_accurate: This is also based on the You Look Only Once AI model with some modifications to make it relatively faster but with the cost of losing accuracy. we do not recommend using this model because it is slower than ssd and in most cases ssd outperforms it, only use it if the object you want to detect is not available in ssd or if you noticed it is better than ssd in a certain circumstance.</p> <p>Notes: You do not need a GPU in any of these models</p> show_frames <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: if false the display window will not show. this will be useful if you want to use less processing power.</p> <p>Notes: if you want to use the code for a certain project with the Raspberry pi 4 (or any lny linux machine) and would like the code to immediately run once you power the processor, then follow the steps in the readMe.md file which will be generated if you click on the \"Generate Code\" in the \"Control Window\" and make sure to make this variable false.</p> resize <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: This variable simply will allow you to scale your frame. if it is false, then the scale variable will not have any effect</p> scale <p>type: float possible values (depends on resize variable): more than 0 and preferred to be less than 1 default value: 1  </p> <p>Explanation: This variable is the percentage scale of your original frame. if you have a frame of 1000 pixels width and 1000 pixels height and you make scale variable to 0.5 then you will have 500 pixels for height and width. this variable  will be useful if you have a frame with very high resolution and you want to make your program run faster.</p> <p>Notes: This variable depends on the resize variable. if it is false then its value will have no effect. It will not make sense if you make this variable more than 1 since it will not increase the resolution but simply will make your program much slower since you have increased the number of pixels.  </p> delay_time <p>type: int possible values: 1 and more (preferred 1) default value: 1  </p> <p>Explanation: time in milliseconds it waits after each frame.  </p> ASCII_close_window_key <p>type: int possible values: 0 to 127 (preferred 27) default value: 27  </p> <p>Explanation: the key on your keyboard that will close the program. every key on your keyboard has an ASCII value. the ASCII for the Esc key is 27</p> confidence SSD_config_file SSD_model SSD_labels YOLO_v3_accurate_config_file YOLO_v3_accurate_model YOLO_v3_accurate_blob_size YOLO_v3_not_accurate_config_file YOLO_v3_not_accurate_model YOLO_v3_not_accurate_blob_size YOLO_v3_display_border_thickness <p>type: float possible values: more than 0 and less than 1 (preferred 0.003) default value: 0.003  </p> <p>Explanation: the thickness of the rectangle (or border) that will be displayed on the frame on a detected object for the YOLO model. This number represents the proportion of the hight of the frame. for example if the frame hight is 1000 pixels and the YOLO_v3_display_border_thickness variable is 0.003 then the thickness will be 3 pixels. it is relative to the hight of the frame so that the display will not change if the resolution changes.</p> YOLO_v3_display_text_size <p>type: float possible values: more than 0 and less than 1 (preferred 0.0015) default value: 0.0015  </p> <p>Explanation: the size of the text that will display the object name. This number represents the proportion of the hight of the frame. for example if the frame hight is 1000 pixels and the YOLO_v3_display_text_size variable is 0.005 then the thickness will be 5 units. it is relative to the hight of the frame so that the display will not change if the resolution changes.</p> YOLO_v3_display_text_thickness <p>type: float possible values: more than 0 and less than 1 (preferred value 0.0025) default value: 0.0025  </p> <p>Explanation: the thickness of the text that will display the object name. this number represents the proportion of of the hight of the frame. for example if the frame hight is 1000 pixels and the YOLO_v3_display_text_thickness variable is 0.0025 then the thickness will be 2.5 units. it is relative to the hight of the frame so that the display will not change if the resolution changes.</p> SSD_display_border_thickness <p>type: float possible values: more than 0 and less than 1 (preferred 0.003) default value: 0.003  </p> <p>Explanation: the thickness of the rectangle (or border) for the SSD model that will be displayed on the frame on the detected object. this number represents the proportion of of the hight of the frame. for example if the frame hight is 1000 pixels and the YOLO_v3_display_border_thickness variable is 0.003 then the thickness will be 3 pixels. it is relative to the hight of the frame so that the display will not change if the resolution changes.</p> location <p>type: string possible values: any string default value: KFUPM  </p> <p>Explanation: This variable will be one of the results that will be recorded once a detection was found.  </p> <p>Use case: one of the useful cases is when this software is used in a surveillance application. Say that you decided to use 1000 cameras in a certain area. multiple cameras or each one is connected to a computer that uses the software. you can make the location variable for each camera describes exactly the place it is in such as using the x and y GPS coordinate. Another example is if you have a drone you could update the location variable as the drone moves so that you could know the exact coordinate of each detection you had.</p> font_TTF_file <p>type: string possible values: .ttf file path default value: resources/fonts/times.ttf (The Times New Roman Font)  </p> <p>Explanation: the font that will be used in displaying most of the texts in the software.  </p> display_text_sizes <p>type: python dictionary (or javascript object) with 5 variables each is a float possible values: every variable in display_text_sizes could be from 0 to 100 default value: {\"fps\":5,\"location\":5,\"SSD_objects\":5,\"date_and_time\":5,\"AI_model\": 5}  </p> <p>Explanation: the 5 variables will be displayed in the frame. their values represent their size percentage relative to the frame hight. so if the frame hight is 1000 pixels and for an example fps is 10 then it will be 10% of the hight of the frame which is 100 pixels height.</p> <p>Notes: if you want to remove one of the 5 texts displayed in your frame then make the its variable equals to 0.</p> display_text_colors <p>type: python dictionary (or javascript object) with 5 variables each has a list of three integers possible values: every variable in the font_sizes has a list that has three values each could be from 0 to 255 default value: [255,0,0] for all (blue)  </p> <p>Explanation: the values in the list inside the variables inside the font_sizes describes the color of the text. the color order is BGR (Blue, green, Red). For example if fps is [0,0,255] then it will be red or if it is [0,255,255] it will be yellow and so on.</p> display_text_locations <p>type: python dictionary (or javascript object) with 5 variables each has a list of two floats possible values: every variable in font_sizes has a list that has two values each could be from 0 to 1  </p> <p>Explanation: the values in the list inside the variables inside the font_sizes describes the location of the texts.   </p> <p>the first value is the x axis and the second value is the y axis. the coordinate starts from the upper left. the x axis increases from left to right and the y axis increases from up to bottom. the x and y axes is for the location of the upper left of the text.  </p> <p>for example if fps is [0.5, 0.5] then the upper left point of the text will be at the center.  </p> <p>Note: if you make fps or any variable's x or y axis to 1 then it will disappear because this is the location of the upper left of the text.  </p> display_objects_colors <p>type: list that has 5 values each value should be a string possible values: every string in the list should have three integer values each from 0 to 255  </p> <p>Explanation: This variable is only applicable for the two YOLO models. the values represent colors (B,G,R) or (Blue, Green, Red). for example (255,0,0) means that it is a blue color. these colors will display when an object is detected. display_objects_colors values are related to the YOLO labels. the file path is: resources/labels/YOLOv3_Labels.txt.</p> <p>the YOLOv3_Labels.txt file contains 80 labels (or objects names). the display_objects_colors variable will be mapped with the objects names in the YOLOv3_Labels.txt file so that  every object from the 80 objects will have one of the 5 colors. for example the first object in the YOLOv3_Labels.txt file is \"person\" so its color will be the first color in the five colors the third object which is car will have the third color of the five colors in the display_objects_colors. the sixth object which is \"bus\" will have the first color of the five colors the seventh will have the second color of the five and so on.</p> occurrence_interval_frames <p>type: int possible values: 1 or more  </p> <p>Explanation: after a certain object is detected the program will save the image and wait a number of frames equal to the occurrence_interval_frames variable (the waiting includes the saved image) than saves again and so on. </p> <p>For an example if occurrence_interval_frames is 10 and a car was detected at the first frame than the program will count frame one until 10 then will start searching again at frame 11 for the car object to save it if it is available.</p> <p>Another example is if occurrence_interval_frames is 10 and a car was first detected at frame 15 and then 18,19,22 then 25 and a person was first detected at frame 16 then 17, 18, 19 and 100 then the frames that will be saved are 15, 16, 25 and 100.</p> <p>Use case: this variable will be useful in case you want to save the frames which detected what you want but not every single one to reduce the size of your results. for example if there is a car which is not moving in the scene and the program was able to detect it in every single frame and the program is runs at 10 fps then this means that in only one hour there will be  106060 = 6000 images saved which is a lot.</p> <p>Notes: make sure that the save_results and save_images variables are true otherwise the images will not be saved. the occurrence_interval_frames is for every object not every detection as it has been explained  in the second example. </p> save_results <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: if there was a detection found then this variable will save the results in the Results folder.  </p> <p>Notes: a .csv file will always be present if this value is true. if this value is false then the save_images will not have effect even if it true.  </p> save_images <p>type: boolean possible values (depends on save_results): true or false default value: true  </p> <p>Explanation: if this variable is true then there will be images saved in the Results folder. make this variable false if you want to save space.  </p> <p>Notes: if the save_results variable is false then the save_images variable will not save images even if it ia true  </p> add_results_on_image <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: if false the results images in the Results folder will not display some results such as frames per seconds and date and time, and location etc.</p> <p>Notes: only the saved images will not show if this variable is false. the window that will display the</p> YOLO_v3_objects_chosen <p>type: list with string values possible values: any values in the object_detection/resources/labels/YOLOv3_Labels.txt file.</p> <p>Explanation: if you choose one of the two YOLO models, then you can choose one of the objects out of the 80 in the YOLOv3_Labels.txt to be detected. if you choose nothing then nothing will be detected if you choose car and person then only these two will bwe detected</p> <p>Notes: do not change the YOLOv3_Labels.txt file. if you do you will get wrong results the names of the objects should exactly be the same as in the YOLOv3_Labels.txt file or you will get an error.  </p> SSD_objects_chosen <p>type: list with string values possible values: any values in the object_detection/resources/labels/SSD_Labels.txt.txt file.  </p> <p>Explanation: if you choose the SSD model, then you can choose one of the objects in the SSD_Labels.txt to be detected. if you choose nothing then nothing will be detected if you choose car and person then only these two will bwe detected</p> <p>Notes: do not change the SSD_Labels.txt file. if you do you will get wrong results. this file used to have a lot of switched objects labels but a lot were fixed. if you choose a unpopular object then you might get another object name.</p> add_to_previous_results <p>type: boolean possible values: true or false default value: false  </p> <p>Explanation: if this value is true and you close and open the program multiple times then the .csv file in the Results folder will not change (the results will be accumulated)</p> add_header_to_next_results <p>type: boolean possible values (depends on add_to_previous_results): true or false. default value: true  </p> <p>Explanation: this variable will add a header in the .csv file in the Results folder to the accumulated results (if add_to_previous_results is true and you have open the program more than two times).</p> image_extension <p>type: string possible values: jpg or png default value: jpg   </p> <p>Explanation: the formate of the image that will be saved. use .jpg if you want less size.  </p> results results[\"objects\"] <p>type: python dictionary  </p> <p>Example: {'person': [4, 0.713], 'car': [1, 0.82]}  </p> <p>Example Explanation:  </p> <p>The keys of the dictionary are the object detected for the frame in the detect function. the value of any key is a list that consist of two numbers. the first number is the number of  objects that were detected in that frame. the second numbers is the maximum confidence of the objects. in the example there are 4 people that were detected in the frame and out of the 4 the highest confidence is 71.3%. the second object is only one car so obviously that car confidence is 82% (the software is 82% sure that it is a car)</p> results[\"location\"] <p>type: string  </p> <p>Example: KFUPM  </p> <p>Example Explanation:  </p> <p>This is simply one of the inputs that is also used as an output for getting more information about the results.  </p> results[\"date_and_time\"] <p>type: string  </p> <p>Example: 2022/08/03, Wed, 04:46:40 PM  </p> <p>Example Explanation:  </p> <p>the date and time there were detection on the frame.  </p> <p>year/month/day, day name, hour:minute:seconds AM or PM  </p> results[\"frame_number\"] <p>type: int  </p> <p>Example: 5937  </p> <p>Example Explanation:  </p> <p>the frame number since the beginning of the program.  </p> results[\"fps\"] <p>type: float  </p> <p>Example: 5.58  </p> <p>Example Explanation:  </p> <p>the frames per second at the time of detection.  </p> results[\"AI_model\"] <p>type: string  </p> <p>Example: SSD  </p> <p>Example Explanation:  </p> <p>This is simply one of the inputs that is also used as an output for getting more information about the results.  </p> results[\"objects_summary\"] <p>type: string  </p> <p>Example: {'person': [14, 15, 0.613], 'cat': [6, 11, 0.83], 'car': [4, 21, 0.942]}  </p> <p>Example Explanation:  </p> <p>this result gives the details of the detected objects since the beginning of the program (unlike results[\"objects\"] which only describes the detected objects in the single frame)  </p> <p>the keys of the dictionary are for the detected objects since the beginning of the program. every key has a list of three values.  </p> <p>the first value is how mush a particular object has occurred since the beginning of the program. this value depends on the occurrence_interval_frames input variable in the .json file. for example if the occurrence_interval_frames is 10 and if the program detects a person on frame number 1 until 10 then the object will still be considered that it has only occurred once. if a person was detected in th 11th or any frame after then this will be considers as the second occurrence. the third occurrence will be 10 frames after the second occurrence and the fourth will be 10 frames after the third and so on. another example is if a car was detected at frame 1,2,6,8,15,100,200,201,204 and 2492, then it will be considered that it has occurred 5 times if occurrence_interval_frames is 10.</p> <p>the second value is the maximum number of objects that were detected from the beginning of the program (this is similar to the first value in the list in results[\"objects\"] but instead of one frame it will consider all of the frames and then get the maximum number).</p> <p>the third value is the maximum confidence of the objects since the beginning of the program (this is similar to the second value in the list in results[\"objects\"] but instead of one frame it will consider all of the frames and then get the maximum number).</p> <p>for the example lets consider the car object. one possible scenario for the first value is if the car was detected at frame number 1,2,3,4 11, 55, and 100. the second value means that out of the seven frames there was a frame were 21 cars were detected and this is the maximum number of the seven frames. the last value means that for all of the detected cars in the whole program the is a car were the program is 94.2% sure that it is a car and this is the maximum number.</p> results_frames <p>type: numpy object.   </p> <p>possible values: three or sometimes two dimensional array (hight, width, and sometimes color channels with three values which are usually (b,g,r)). for the three or one channel (two dimensional array) the values are usually from 0 to 255 (uint8 type in the numpy library)  </p> <p>Notes:   1-the \"type\" and \"possible values\" are true for all of the BV software, but to be more specific for this specific project (object_detection) it is three dimensional array with three (b,g,r) channels.   2-the names of the frames (or one frame) should be self explanatory so we will not go deep in explaining them. Some one with basic knowledge with the numpy library should be able to deal with them without issues.</p> <p>frames:   results_frames[\"original_with_results\"] </p>"},{"location":"documentation/object_detection/#_1","title":"Object Detection","text":"<p>type: int or string (only in the *.json file in the settings folder) possible values (depends on input variable): 1 (or video), 2 (or image), 3 (or camera), 4 (or ip). default value: camera  </p> <p>Explanation: Determine what kind of input the program will use. There are four possible integer values for this variable. They are: 1 (video), 2 (image), 3 (camera), and 4 (ip). more details are in the input variable documentation.  </p> <p>Notes: For your convenience it is possible to use string type as an input but only in the *.json file in the settings folder. for example instead of writing 3 you could write camera or Camera in the default.json file. the load_settings function in your main.py file will take care of converting the input_type variable to an integer.  </p> <p>Also note that the input variable depends on the input_type variable. So if you change the input_type variable you should make sure that the input variable will have a correct value. for example if the input_type variable is a camera (or number 3), then the input variable should only be an integer which represent which camera in your device such as 1. Another example if the input_type is video (or number 1) then the input should be a file path such as C:/filename.mp4</p>"},{"location":"documentation/object_detection/#_2","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_3","title":"Object Detection","text":"<p>type: int, string possible values (depends on input_type variable):  input_type is 1 or 2 (video or image): possible values are file paths (Example: C:/filename.mp4 or C:/filename.png). input_type is 3 (camera): possible values are integers from 0 or more (Example: 2). input_type is 4 (ip): possible values are Live stream URLs (Example: https://192.168.100.25:8080/video).  </p> <p>default value: 0 (for camera)  </p> <p>Explanation: This variable depends on the input_type variable. Here is a brief description of the variable:  </p> <p>input_type is video or image: The input will be a path of the image or video you want to use in you program. usually we use the video or image for testing purposes.  </p> <p>input_type is camera: The input variable will be an integer that will describe what camera you are using (note that the count starts from 0 not 1). So if you have three cameras on your device and write 2 in the *.json file, then you will use the third camera. if you write 3 then you will get an error.</p> <p>input_type IP: The live stream ip address (URL) you want to use in your program. you could stream from your phone if you use the IP Webcam App for Android devices. If you install the app, open it and scroll to the bottom and select \"Start server\". Your live IP server in the local area network will be after \"IPv4:\" (you could choose the first or second). the URL that you need to past is what is after \"IPv4:\" plus \"/video\" (Example: http://192.168.100.25:8080/video) make sure you are at the same local area network. you can test if the stream is available by simply writing the URL in your web browser such as Chrome or Firefox.</p> <p>Notes: If you are using Windows and the input is for an image or video use \"/\" instead of \"\\\" for the path of the video or image or add additional \"\\\" to \"\\\". For example C:\\Users\\Ibrahim\\filename.mp4 should instead be C:/Users/Ibrahim/filename.mp4 or C:\\Users\\Ibrahim\\filename.mp4</p> <p>Do not forget to add the extension. Example: filename.mp4 not filename  </p>"},{"location":"documentation/object_detection/#_4","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_5","title":"Object Detection","text":"<p>type: float possible values: 0 to 1 (0% - 100%)  </p> <p>Explanation: the minimum confidence or accuracy accepted for detection. for example if there is a person in a frame and the program is 0.6 (60%) sure of it and the confidence variable is 0.8 (80%) than this person will not be recorded in your results however if the program is 0.95 (95%) sure then the object will be detected</p>"},{"location":"documentation/object_detection/#_6","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_7","title":"Object Detection","text":"<p>type: string possible values: .pbtxt file path  </p> <p>Explanation: configuration file for the SSD model  </p>"},{"location":"documentation/object_detection/#_8","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_9","title":"Object Detection","text":"<p>type: string possible values: .pb file path  </p> <p>Explanation: the trained SSD model  </p>"},{"location":"documentation/object_detection/#_10","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_11","title":"Object Detection","text":"<p>type: string possible values: .txt file path  </p> <p>Explanation: the names of the objects (labels) for the SSD model  </p>"},{"location":"documentation/object_detection/#_12","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_13","title":"Object Detection","text":"<p>type: string possible values: .cfg file path  </p> <p>Explanation: configuration file for the YOLO accurate model  </p>"},{"location":"documentation/object_detection/#_14","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_15","title":"Object Detection","text":"<p>type: string possible values: .weights file path  </p> <p>Explanation: the trained YOLO Accurate model  </p>"},{"location":"documentation/object_detection/#_16","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_17","title":"Object Detection","text":"<p>type: int possible values: 0 or more (preferred 608)  </p> <p>Explanation: not very sure what exactly this variable does. so I have attached a link that I hope could be useful. the link includes the paper for YOLO.  </p> <p>https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html  </p>"},{"location":"documentation/object_detection/#_18","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_19","title":"Object Detection","text":"<p>type: string possible values: .cfg file path  </p> <p>Explanation: configuration file for the YOLO not accurate model  </p>"},{"location":"documentation/object_detection/#_20","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_21","title":"Object Detection","text":"<p>type: string possible values: .weights file path  </p> <p>Explanation: the trained YOLO not accurate model  </p>"},{"location":"documentation/object_detection/#_22","title":"Object Detection","text":""},{"location":"documentation/object_detection/#_23","title":"Object Detection","text":"<p>type: int possible values: more than 0 and less than 1 (preferred 608)  </p> <p>Explanation: not very sure what exactly this variable does. so I have attached a link that I hope could be useful. the link includes the paper for YOLO.  </p> <p>https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html  </p>"},{"location":"documentation/object_detection/#_24","title":"Object Detection","text":""},{"location":"documentation/pipe_angle/","title":"Pipe Angle Detection","text":"notes <p>This project is almost exactly identical to the color_detection project (that was not intentional but for multiple of reasons we have decided to separate them instead of making them a single project).</p> .json file input_type input arrow_length <p>type: float possible values: from 0 to 1 default value: 0.2  </p> <p>Explanation: the length of the displayed arrow for the direction of a pipe or any object. the value is the ratio of the frame hight  </p> arrow_color <p>type: list possible values: the list has three values each could be from 0 to 255 default value: [0,155,255]  </p> <p>Explanation: the bgr color (Blue,Green,Red) of the arrow.  </p> arrow_thickness <p>type: float possible values: from 0 to 1 default value: 0.01  </p> <p>Explanation: the thickens of the displayed arrow. the value is the ratio of the frame hight  </p> centroid_size <p>type: float possible values: from 0 to 1 default value: 0.015  </p> <p>Explanation: the size of the radius of the circle which is the centroid of the largest continuous white area (the white area is the detected color range)</p> centroid_color <p>type: list possible values: the list has three values each could be from 0 to 255 default value: [0,0,255]  </p> <p>Explanation: the bgr color (Blue,Green,Red) of the centroid circle  </p> contour_s_color <p>type: list possible values: the list has three values each could be from 0 to 255 default value: [0,0,255]  </p> <p>Explanation: the bgr color (Blue,Green,Red) of the border (or perimeter) of the largest continuous white color (the color that represent the detected range)  </p> contour_s_thickness <p>type: float possible values: from 0 to 1 default value: 0.01  </p> <p>Explanation: the thickness of the border (or perimeter) of the largest continuous white color (the color that represent the detected range).  the value is the ratio of the frame hight.</p> lowerH, upperH, lowerS, upperS, lowerV, and upperV <p>type: int possible values: from 0 to 360 for lowerH, and upperH. and from 0 to 256 for lowerS, upperS, lowerV, and upperV  </p> <p>Explanation: the lower and upper ranges for the HSV color (HSV stands for hue saturation value)  </p> <p>for more information go to: http://color.lukas-stratmann.com/color-systems/hsv.html or https://medium.com/programming-fever/how-to-find-hsv-range-of-an-object-for-computer-vision-applications-254a8eb039fc  </p> additional_lowerH, additional_upperH, additional_lowerS, additional_upperS, additional_lowerV, and additional_upperV <p>type: list possible values: from 1 to 360 for lowerH, and upperH. and from 1 to 256 for lowerS, upperS, lowerV, and upperV. all of the list should have the same number of inputs or you will get an error.  </p> <p>Explanation: an additional HSV color range (HSV stands for hue saturation value). this will be useful if you want to select two more different colors at the same time.</p> <p>Notes: please note that the lists should have the same number of values or you will get an error. for example if additional_lowerH = [2,77,300,44] (5 inputs), then the rest should have 5 inputs.  </p> <p>for more information go to: http://color.lukas-stratmann.com/color-systems/hsv.html or  https://medium.com/programming-fever/how-to-find-hsv-range-of-an-object-for-computer-vision-applications-254a8eb039fc  </p> blur <p>type: int possible values: from 0 and more (preferred less then 20) default value: 4  </p> <p>Explanation: This filter the image and makes it blur. this is one of the filters that will help to get ride of the noise  </p> erosion <p>type: int possible values: from 0 and more (preferred less then 10) default value: 5  </p> <p>Explanation: This filter will decrease all of the white areas which represent the detected color range. this is one of the filters that will help to get ride of the noise.</p> dilation <p>type: int possible values: from 0 and more (preferred less then 10) default value: 2  </p> <p>Explanation: This filter will increase all of the white areas which represent the detected color range. this filter will be applied after the erosion filter. it will be useful if for example there are two close parallel objects such as two pips and we want to analyze the two as a one object. also this will solve the issue of having two objects with almost identical areas which will make the program alternate between the two objects.</p> contourAreaRatio <p>type: float possible values: from 0 to 1 (preferred 0.005) default value: 0.005  </p> <p>Explanation: if the ratio of the white color pixels (the detected color range) to the frame (hight * width) pixels is greater than this value then it will start recording the detection.</p> show_frames <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: if false the display window will not show. this will be useful if you want to use less processing power.  </p> <p>Notes: if you want to use the code for a certain project with the Raspberry pi 4 (or any lny linux machine) and would like the code to immediately run once you power the processor, then follow the steps in the readMe.md file which will be generated if you click on the \"Generate Code\" in the \"Control Window\" and make sure to make this variable false.</p> resize scale delay_time ASCII_close_window_key location <p>type: string possible values: any string default value: KFUPM  </p> <p>Explanation: This variable will be one of the results that will be recorded once a detection was found.  </p> <p>Use case: one of the useful cases is when this software is used in a surveillance application. Say that you decided to use 1000 cameras in a certain area. multiple cameras or each one is connected to a computer that uses the software. you can make the location variable for each camera describes exactly the place it is in such as using the x and y GPS coordinate. Another example is if you have a drone you could update the location variable as the drone moves so that you could know the exact coordinate of each detection you had.</p> font_TTF_file <p>type: string possible values: .ttf file path default value: resources/fonts/times.ttf (The Times New Roman Font)  </p> <p>Explanation: the font that will be used in displaying most of the texts in the software.  </p> display_text_sizes <p>type: python dictionary (or javascript object) with 5 variables each is a float possible values: every variable in display_text_sizes could be from 0 to 100 default value: {\"fps\":5,\"location\":5,\"SSD_objects\":5,\"date_and_time\":5,\"AI_model\": 5}  </p> <p>Explanation: the 5 variables will be displayed in the frame. their values represent their size percentage relative to the frame hight. so if the frame hight is 1000 pixels and for an example fps is 10 then it will be 10% of the hight of the frame which is 100 pixels height.  </p> <p>Notes: if you want to remove one of the 5 texts displayed in your frame then make the its variable equals to 0.</p> display_text_colors <p>type: python dictionary (or javascript object) with 5 variables each has a list of three integers possible values: every variable in the font_sizes has a list that has three values each could be from 0 to 255 default value: [255,0,0] for all (blue)  </p> <p>Explanation: the values in the list inside the variables inside the font_sizes describes the color of the text. the color order is BGR (Blue, green, Red). For example if fps is [0,0,255] then it will be red or if it is [0,255,255] it will be yellow and so on.</p> display_text_locations <p>type: python dictionary (or javascript object) with 5 variables each has a list of two floats possible values: every variable in font_sizes has a list that has two values each could be from 0 to 1  </p> <p>Explanation: the values in the list inside the variables inside the font_sizes describes the location of the texts.  </p> <p>the first value is the x axis and the second value is the y axis. the coordinate starts from the upper left. the x axis increases from left to right and the y axis increases from up to bottom. the x and y axes is for the location of the upper left of the text.  </p> <p>for example if fps is [0.5, 0.5] then the upper left point of the text will be at the center.  </p> <p>Note: if you make fps or any variable's x or y axis to 1 then it will disappear because this is the location of the upper left of the text.  </p> save_results <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: if there was a detection found then this variable will save the results in the Results folder.  </p> <p>Notes: a .csv file will always be present if this value is true. if this value is false then the save_images will not have effect even if it true.  </p> save_images <p>type: boolean possible values (depends on save_results): true or false default value: true  </p> <p>Explanation: if this variable is true then there will be images saved in the Results folder. make this variable false if you want to save space.  </p> <p>Notes: if the save_results variable is false then the save_images variable will not save images even if it ia true  </p> add_to_previous_results <p>type: boolean possible values: true or false default value: false  </p> <p>Explanation: if this value is true and you close and open the program multiple times then the .csv file in the Results folder will not change (the results will be accumulated)  </p> add_header_to_next_results <p>type: boolean possible values (depends on add_to_previous_results): true or false. default value: true  </p> <p>Explanation: this variable will add a header in the .csv file in the Results folder to the accumulated results (if add_to_previous_results is true and you have open the program more than two times).</p> image_extension <p>type: string possible values: jpg or png default value: jpg  </p> <p>Explanation: the formate of the image that will be saved. use .jpg if you want less size.</p> Info <p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: if false the results images in the Results folder will not display some results such as frames per seconds and date and time, and location etc.  </p> <p>Notes: only the saved images will not show if this variable is false. the window that will display the</p> results results[\"frame_number\"] <p>type: int</p> <p>Example: 478</p> <p>Example Explanation:</p> <p>the frame number since the beginning of the program.</p> results[\"fps\"] <p>type: float</p> <p>Example: 5.58</p> <p>Example Explanation:</p> <p>the frames per second at the time of detection.</p> results[\"location\"] <p>type: string</p> <p>Example: KFUPM</p> <p>Example Explanation:</p> <p>This is simply one of the inputs that is also used as an output for getting more information about the results.</p> results[\"date_and_time\"] <p>type: string</p> <p>Example: 2022/08/03, Wed, 04:46:40 PM</p> <p>Example Explanation:</p> <p>the date and time there were detection on the frame.</p> <p>year/month/day, day name, hour:minute:seconds AM or PM</p> results[\"x\"] <p>type: float</p> <p>Example: 0.50</p> <p>Example Explanation:</p> <p>the normalized x coordinate of the centroid of the contour area (the white area which represent the color range). the 0.5 means that it is in the middle of the frame. the x axis starts from the left and increase right.</p> results[\"y\"] <p>type: float</p> <p>Example: 0.9</p> <p>Example Explanation:</p> <p>the normalized y coordinate of the centroid of the contour area (the white area which represent the color range). the 0.9 means that it is almost at the top. the x axis starts from the bottom and increase upp</p> results[\"angle\"] <p>type: float</p> <p>Example: 95.5</p> <p>Example Explanation:</p> <p>the angle of the white largest continuos area which represent the detection for the color rang. the possible values for the angle are from 0 to 180. the angle starts from the positive x axis which means that it will always be directed upwards</p> results_frames <p>type: numpy object.  </p> <p>possible values: three or sometimes two dimensional array (hight, width, and sometimes color channels with three values which are usually (b,g,r)). for the three or one channel (two dimensional array) the values are usually from 0 to 255 (uint8 type in the numpy library)</p> <p>Notes: 1-the \"type\" and \"possible values\" are true for all of the BV software. 2-the names of the frames (or one frame) should be self explanatory so we will not go deep in explaining them. Some one with basic knowledge with the numpy library should be able to deal with them without issues.</p> <p>frames: results_frames[\"1_original_with_results\"] results_frames[\"2_filtered\"] results_frames[\"3_HSV_range(s)\"] results_frames[\"4_erosion\"] results_frames[\"5_dilation\"] results_frames[\"6_contour(s)_centroid\"]  </p>"},{"location":"documentation/pipe_angle/#_1","title":"Pipe Angle Detection","text":"<p>type: int or string (only in the *.json file in the settings folder) possible values (depends on input variable): 1 (or video), 2 (or image), 3 (or camera), 4 (or ip). default value: camera  </p> <p>Explanation: Determine what kind of input the program will use. There are four possible integer values for this variable. They are: 1 (video), 2 (image), 3 (camera), and 4 (ip). more details are in the input variable documentation.  </p> <p>Notes: For your convenience it is possible to use string type as an input but only in the *.json file in the settings folder. for example instead of writing 3 you could write camera or Camera in the default.json file. the load_settings function in your main.py file will take care of converting the input_type variable to an integer.  </p> <p>Also note that the input variable depends on the input_type variable. So if you change the input_type variable you should make sure that the input variable will have a correct value. for example if the input_type variable is a camera (or number 3), then the input variable should only be an integer which represent which camera in your device such as 1. Another example if the input_type is video (or number 1) then the input should be a file path such as C:/filename.mp4</p>"},{"location":"documentation/pipe_angle/#_2","title":"Pipe Angle Detection","text":""},{"location":"documentation/pipe_angle/#_3","title":"Pipe Angle Detection","text":"<p>type: int, string possible values (depends on input_type variable): input_type is 1 or 2 (video or image): possible values are file paths (Example: C:/filename.mp4 or C:/filename.png). input_type is 3 (camera): possible values are integers from 0 or more (Example: 2). input_type is 4 (ip): possible values are Live stream URLs (Example: https://192.168.100.25:8080/video).  </p> <p>default value: 0 (for camera)  </p> <p>Explanation: This variable depends on the input_type variable. Here is a brief description of the variable:  </p> <p>input_type is video or image: The input will be a path of the image or video you want to use in you program. usually we use the video or image for testing purposes.  </p> <p>input_type is camera: The input variable will be an integer that will describe what camera you are using (note that the count starts from 0 not 1). So if you have three cameras on your device and write 2 in the *.json file, then you will use the third camera. if you write 3 then you will get an error.</p> <p>input_type IP: The live stream ip address (URL) you want to use in your program. you could stream from your phone if you use the IP Webcam App for Android devices. If you install the app, open it and scroll to the bottom and select \"Start server\". Your live IP server in the local area network will be after \"IPv4:\" (you could choose the first or second). the URL that you need to past is what is after \"IPv4:\" plus \"/video\" (Example: http://192.168.100.25:8080/video) make sure you are at the same local area network. you can test if the stream is available by simply writing the URL in your web browser such as Chrome or Firefox.</p> <p>Notes: If you are using Windows and the input is for an image or video use \"/\" instead of \"\\\" for the path of the video or image or add additional \"\\\" to \"\\\". For example C:\\Users\\Ibrahim\\filename.mp4 should instead be C:/Users/Ibrahim/filename.mp4 or C:\\Users\\Ibrahim\\filename.mp4</p> <p>Do not forget to add the extension. Example: filename.mp4 not filename</p>"},{"location":"documentation/pipe_angle/#_4","title":"Pipe Angle Detection","text":""},{"location":"documentation/pipe_angle/#_5","title":"Pipe Angle Detection","text":"<p>type: boolean possible values: true or false default value: true  </p> <p>Explanation: This variable simply will allow you to scale your frame. if it is false, then the scale variable will not have any effect</p>"},{"location":"documentation/pipe_angle/#_6","title":"Pipe Angle Detection","text":""},{"location":"documentation/pipe_angle/#_7","title":"Pipe Angle Detection","text":"<p>type: float possible values (depends on resize variable): more than 0 and preferred to be less than 1 default value: 1  </p> <p>Explanation: This variable is the percentage scale of your original frame. if you have a frame of 1000 pixels width and 1000 pixels height and you make scale variable to 0.5 then you will have 500 pixels for height and width. this variable  will be useful if you have a frame with very high resolution and you want to make your program run faster.  </p> <p>Notes: This variable depends on the resize variable. if it is false then its value will have no effect. It will not make sense if you make this variable more than 1 since it will not increase the resolution but simply will make your program much slower since you have increased the number of pixels.  </p>"},{"location":"documentation/pipe_angle/#_8","title":"Pipe Angle Detection","text":""},{"location":"documentation/pipe_angle/#_9","title":"Pipe Angle Detection","text":"<p>type: int possible values: 1 and more (preferred 1) default value: 1  </p> <p>Explanation: time in milliseconds it waits after each frame.  </p>"},{"location":"documentation/pipe_angle/#_10","title":"Pipe Angle Detection","text":""},{"location":"documentation/pipe_angle/#_11","title":"Pipe Angle Detection","text":"<p>type: int possible values: 0 to 127 (preferred 27) default value: 27  </p> <p>Explanation: the key on your keyboard that will close the program. every key on your keyboard has an ASCII value. the ASCII for the Esc key is 27</p>"},{"location":"documentation/pipe_angle/#_12","title":"Pipe Angle Detection","text":""},{"location":"examples/color_detection/","title":"Color Detection","text":""},{"location":"examples/color_detection/#example-1","title":"Example 1","text":"The \"Get Largest Continuous Area\" will make the results only based on the largest continuous area (the white or detected range). As you can see when it is selected the normalized centroid will only be at the largest circle. The other parameter used is the \"Detect Area Ratio Above\". This parameter will see if the Area ratio which is the white area over the black area is greater then its value or not. if it is, then it will start giving values or save the detection details to the .csv file (if the \"Save Results\" is selected). Note that if the Get Largest Continuous Area is selected then the area ratio will be less since the other three circle will not be included."},{"location":"examples/color_detection/#example-2","title":"Example 2","text":"The example shows you how to deal with an extreme case where if you want to select an object (get its centroid) and it consist of multiple colors and there are other small objects around it with the same color (there is noise) and it is not the only object with the same color. Note that in the example we made the object of two colors but of course if it is composed of three or even more colors, we could simply add a third or a fourth color and go with the same procedure."},{"location":"examples/object_detection/","title":"Object Detection","text":""},{"location":"examples/object_detection/#example-1","title":"Example 1","text":"In the example you could choose an AI model you want to use for detection and the objects you want to detect. Not only that but also the results will be displayed as a bar chart. There are three parts to the bar chart. First is its height which is the \"Object Occurrence\" which is the time the object was saved as an image or would have been saved if the \"Save Images\" is on  (this is directly related to the \"Frames Skipped\" in the \"User Guide\"). The remaining two will appear when you hover over a certain bar. One is the maximum time an object appeared in one frame. Notice that the person is two because there are two people. And the last one is the maximum confidence of the certain object since the beginning of the program. Note that since the YOLO version 3 was used on a car after the SSD, the maximum confidence of the car became 99.5 on the bar. These results will be one of the results to be saved the .csv file. They could be extremely important if for an example you have a lot of cameras and you make them all record for days for the object detection project. It might be a waste of your valuable time to go through the images which contain a detected object or objects one by one. Instead, you could have a simple graph which will describe to you what happen for all the time period.  Note that the YOLO version 7 AI model is now available . So, it might be included in the future."},{"location":"examples/pipe_angle/","title":"Pipe Angle Detection","text":"Notes: <p> 1. This project is very similar to the \"Color Detection\" project. So if do not understand some of its parameters then simply refer to the \"Color Detection\" project. The examples below simply shows some use cases or issues that could be addressed related to this project. </p>"},{"location":"examples/pipe_angle/#example-1","title":"Example 1","text":"The dilation will simply increase the white area (detected part).  This parameter could be very useful when you are trying to detect a pipe that is not 100% visible or maybe you have two or three pipes and you want to detect them, but you do not want the detection to alter between the pipe so that you will not get inconsistent results so."},{"location":"results/color_detection/","title":"Color Detection","text":""},{"location":"results/color_detection/#oil-leakage","title":"Oil leakage","text":""},{"location":"results/object_detection/","title":"Object Detection","text":""},{"location":"results/object_detection/#ssd-and-yolo-version-3-ai-models","title":"SSD and YOLO version 3 AI models","text":""},{"location":"results/pipe_angle/","title":"Pipe Angle Detection","text":""},{"location":"results/pipe_angle/#pipe-angle","title":"Pipe Angle","text":""},{"location":"results/pipe_angle/#flow-angle","title":"Flow Angle","text":""},{"location":"results/pipe_angle/#angle-detection-with-motion","title":"Angle Detection with Motion","text":""}]}